# Линейная фильтрация изображений (горизонтальное разбиение). Ядро Гаусса 3x3

- Студент: Лазарева Анна Анатольевна, группа 3823Б1ПР1
- Технология: MPI + SEQ
- Вариант: 26

## 1. Введение

Цифровая обработка изображений является важной областью компьютерных наук, находящей применение в медицинской диагностике, системах компьютерного зрения, фотографии и многих других сферах. Одной из базовых операций обработки изображений является фильтрация, позволяющая улучшать качество изображений, удалять шум и выделять важные признаки.

Гауссов фильтр — один из наиболее широко используемых линейных фильтров для сглаживания изображений. Он основан на двумерной функции Гаусса и обеспечивает эффективное подавление высокочастотного шума при сохранении общей структуры изображения. Ядро размером 3×3 представляет собой компромисс между качеством сглаживания и вычислительной эффективностью.

При обработке изображений высокого разрешения последовательное применение фильтра становится вычислительно затратным. Параллельная обработка позволяет существенно ускорить вычисления за счёт распределения работы между несколькими процессорами. Горизонтальное разбиение изображения (по строкам) является естественным способом декомпозиции данной задачи, обеспечивающим равномерное распределение нагрузки и минимизацию обмена данными между процессами.

В данной работе реализован алгоритм линейной фильтрации изображений с ядром Гаусса 3×3, использующий горизонтальное разбиение данных и технологию MPI для параллельного выполнения на распределённых вычислительных системах.

## 2. Постановка задачи

**Определение задачи:** Определение задачи: Дано полутоновое изображение размером height × width пикселей. Необходимо применить линейную фильтрацию с ядром Гаусса 3×3 ко всем пикселям изображения, используя параллельную обработку с горизонтальным разбиением.

*Ядро Гаусса 3×3:*
     | 1  2  1 |
K =  | 2  4  2 |  ×  1/16
     | 1  2  1 |
Сумма весов ядра: 1+2+1+2+4+2+1+2+1 = 16

*Операция свёртки:*
Для каждого пикселя (i, j) выходного изображения:
O(i,j) = Σ Σ I(i+ki, j+kj) × K(ki+1, kj+1) / 16
         ki=-1..1  kj=-1..1

**Формат входных данных:**
- Входной вектор: `[height, width, pixel[0], pixel[1], ..., pixel[height×width-1]]`
- `height` — высота изображения (количество строк)
- `width` — ширина изображения (количество столбцов)
- `pixel[i]` — значение пикселя (0-255 для полутонового изображения)

**Формат выходных данных:**
- Выходной вектор: `[filtered_pixel[0], filtered_pixel[1], ..., filtered_pixel[height×width-1]]`

**Обработка границ:**
- Используется метод clamping (повторение крайних пикселей):
 - Если координата выходит за границу, используется ближайший граничный пиксель

 ```cpp
 int row = std::clamp(i + ki - 1, 0, height - 1);
 int col = std::clamp(j + kj - 1, 0, width - 1);
 ```

**Ограничения:**
- Размеры изображения: `height` > 0, `width` > 0
- Значения пикселей: 0 ≤ `pixel[i]` ≤ 255
- Размер входных данных должен соответствовать: size = 2 + height × width

**Пример:**
Входное изображение 3×3:
| 100  100  100 |
| 100  200  100 |
| 100  100  100 |

После применения фильтра Гаусса центральный пиксель:
(1×100 + 2×100 + 1×100 + 2×100 + 4×200 + 2×100 + 1×100 + 2×100 + 1×100) / 16
= (100 + 200 + 100 + 200 + 800 + 200 + 100 + 200 + 100) / 16
= 2000 / 16 = 125


## 3. Описание базового алгоритма (последовательная версия)

**Шаги алгоритма:**
1. **Разбор входных данных:** Извлечение `height`, `width` и массива пикселей
2. **Валидация:** Проверка корректности размеров и соответствия размера данных
3. **Применение фильтра:** 
   - Вычисление взвешенной суммы соседних пикселей с ядром Гаусса
   - Обработка граничных условий методом clamping
   - Нормализация результата делением на сумму весов ядра
4. **Возврат:** Выходной вектор отфильтрованных пикселей

**Сложность:**
- Время: O(height × width × 9) = O(n), где n = height × width
- Память: O(n) для входных данных + O(n) для выходных данных


## 4. Схема распараллеливания

**Горизонтальное разбиение:**
Изображение разбивается на горизонтальные полосы (группы строк), которые распределяются между процессами.
Изображение 8×6, 4 процесса:

Процесс 0: строки 0-1  (2 строки)
Процесс 1: строки 2-3  (2 строки)
Процесс 2: строки 4-5  (2 строки)
Процесс 3: строки 6-7  (2 строки)
**Распределение строк:**

rows_per_proc = height / size;
remainder = height % size;

// Процесс i получает:
local_rows = rows_per_proc + (i < remainder ? 1 : 0);
local_start = i * rows_per_proc + min(i, remainder);

**Проблема граничных строк:**
Для применения фильтра 3×3 каждому процессу нужны соседние строки (halo rows):
- Первая строка процесса требует последнюю строку предыдущего процесса
- Последняя строка процесса требует первую строку следующего процесса
*Решение: включение halo-строк в Scatterv*
Вместо отдельного обмена граничными строками, они включаются непосредственно в операцию распределения данных:

// Для каждого процесса i:
halo_top = (start > 0) ? 1 : 0;
halo_bottom = (start + count < height) ? 1 : 0;

sendcounts[i] = (count + halo_top + halo_bottom) * width;
displs[i] = (start - halo_top) * width;

**Схема передачи данных:**

MPI_Scatterv:
┌─────────────────┐
│  Полное изображение  │  (rank 0)
└────────┬────────┘
         │
    ┌────┴────┬────────┬────────┐
    ▼         ▼        ▼        ▼
┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐
│ halo  │ │ halo  │ │ halo  │ │ halo  │
│ rows  │ │ rows  │ │ rows  │ │ rows  │
│ 0-2   │ │ 1-4   │ │ 3-6   │ │ 5-7   │
└───────┘ └───────┘ └───────┘ └───────┘
  rank 0    rank 1    rank 2    rank 3

Локальная обработка (параллельно):
┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐
│filter │ │filter │ │filter │ │filter │
│rows   │ │rows   │ │rows   │ │rows   │
│ 0-1   │ │ 2-3   │ │ 4-5   │ │ 6-7   │
└───────┘ └───────┘ └───────┘ └───────┘

MPI_Gatherv:
    ┌────┴────┬────────┬────────┐
    │         │        │        │
    ▼         ▼        ▼        ▼
┌─────────────────────────────────┐
│     Результат (rank 0)          │
└─────────────────────────────────┘


**MPI-операции:**
- MPI_Bcast — рассылка размеров изображения
- MPI_Scatterv — распределение данных с halo-строками**
- MPI_Gatherv — сбор результатов

**При применении фильтра 3×3 каждый пиксель зависит от 9 соседних пикселей (включая себя). Когда изображение разбивается между процессами, возникает проблема на границах:

Изображение 8×6, разбито на 2 процесса:

Процесс 0 (строки 0-3):          Процесс 1 (строки 4-7):
┌─────────────────────┐          ┌─────────────────────┐
│ строка 0            │          │ строка 4            │ ← Нужна строка 3!
│ строка 1            │          │ строка 5            │
│ строка 2            │          │ строка 6            │
│ строка 3            │ ← Нужна строка 4!  │ строка 7  │
└─────────────────────┘          └─────────────────────┘

Проблема: Чтобы обработать строку 3, процессу 0 нужна строка 4 (которая у процесса 1). И наоборот.

Halo-строки — это дополнительные строки данных, которые копируются с соседних процессов для обеспечения корректной обработки граничных пикселей.
С halo-строками:

Процесс 0:                       Процесс 1:
┌─────────────────────┐          ┌─────────────────────┐
│ строка 0            │          │ строка 3 ← HALO TOP │
│ строка 1            │          │ строка 4            │
│ строка 2            │          │ строка 5            │
│ строка 3            │          │ строка 6            │
│ строка 4 ← HALO BOT │          │ строка 7            │
└─────────────────────┘          └─────────────────────┘

Процесс 0 получает копию строки 4
Процесс 1 получает копию строки 3

## 5. Детали реализации

### Структура кода

**Файлы:**
- `common/include/common.hpp` - определение типов данных
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` - последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` - параллельная реализация
- `tests/functional/main.cpp` - функциональные тесты
- `tests/performance/main.cpp` - тесты производительности

**Классы:**
- `LazarevaAGaussFilterHorizontalSEQ` - последовательная версия (SEQ)
- `LazarevaAGaussFilterHorizontalMPI` - параллельная версия (MPI)

**Методы (одинаковы для обеих реализаций):**
- `ValidationImpl()` - проверка корректности входа
- `PreProcessingImpl()` - подготовка данных
- `RunImpl()` - основная логика (последовательная или MPI).
- `PostProcessingImpl()` - проверка корректности выхода.

### Важные решения при реализации

**Включение halo-строк в Scatterv:**
- Преимущество: минимизация числа MPI-вызовов
- Каждый процесс получает всё необходимое за одну операцию
- Нет дополнительных Send/Recv для обмена границами

**Обработка граничных случаев:**
- Метод clamping* выбран как наиболее простой и эффективный

*Метод clamping (повторение крайних пикселей):
 - Если координата выходит за границу, используется ближайший граничный пиксель

```cpp
 int row = std::clamp(i + ki - 1, 0, height - 1);
 int col = std::clamp(j + kj - 1, 0, width - 1);
```
Исходное изображение:        После clamping:
                             
  ?   ?   ?                  [0,0] [0,0] [0,1]
  ? [0,0] [0,1]      →       [0,0] [0,0] [0,1]
  ? [1,0] [1,1]              [1,0] [1,0] [1,1]

Недостающие пиксели заполняются ближайшими существующими



## 6. Экспериментальное окружение

### 6.1 Конфигурация оборудования

- **CPU:** AMD Ryzen 9 7940HS w/ Radeon 780M Graphics
- **Ядра:** 8 физических ядер (16 логических потоков)
- **ОЗУ:** 8 ГБ DDR4
- **ОС:** WSL Ubuntu 24.04.3 LTS (Linux kernel 6.x)

### 6.2 Программный инструментарий

- **Компилятор:** g++ 11.4.0 
- **Тип сборки:** Release
- **Стандарт C++:** C++20

### 6.3 Тестовое окружение

```bash

PPC_NUM_PROC=2,4,8,16

```

### 6.4 Генерация тестовых данных

**Функциональные тесты:**
- Загрузка реального изображения borsch.jpg
- 5 тестовых случаев с различными размерами кропа: 32×32, 64×64, 128×128, 256×256, 512×512
- Проверка корректности диапазона выходных значений [0, 255]
**Тесты производительности:**
- Размер изображения: 5000×5000 пикселей (25,000,000 пикселей)
- Случайные значения пикселей [0, 255]
- Фиксированный seed для воспроизводимости

## 7. Результаты

### 7.1 Проверка корректности

**Результаты тестирования:**Все функциональные тесты пройдены
**Валидация:**
- Размер выходного вектора соответствует размеру изображения
- Все выходные значения находятся в диапазоне [0, 255]
- Однородное изображение остаётся однородным после фильтрации
- Результаты MPI совпадают с SEQ для одинаковых входных данных

### 7.1 Результаты производительности

**Характеристики задачи:**
- Размер изображения: 5000×5000 = 25,000,000 пикселей
- Количество процессов: 8

#нужна редакция после заупска на норм машине

| Mode         | Count | Time, s | Speedup | Efficiency |
|--------------|-------|---------|---------|------------|
| SEQ          | 1     | 0.1488  | 1.00    | N/A        |
| MPI pipeline | 2     | 0.1469  | 1.01    | 50.5%      |
| MPI task_run | 2     | 0.1286  | 1.16    | 58.0%      |
| MPI pipeline | 4     | 0.1146  | 1.30    | 32.5%      |
| MPI task_run | 4     | 0.0966  | 1.54    | 38.5%      |
| MPI pipeline | 8     | 0.0989  | 1.50    | 18.8%      |
| MPI task_run | 8     | 0.0711  | 2.09    | 26.1%      |


## 8. Выводы

### 8.1 Что сработало хорошо

****
- Правильное вычисление соседей с учётом замыкания краёв
- Оптимальный выбор размеров решётки для произвольного числа процессов

**Эффективный алгоритм маршрутизации**
- XY-routing гарантирует кратчайший путь
- Отсутствие deadlock'ов благодаря dimension-order routing
- Корректная работа для всех пар source/dest

**Надёжная передача данных**
- Протокол с предварительной отправкой размера
- Данные передаются только по пути, а не всем процессам
- Корректная обработка граничных случаев
- Все функциональные тесты пройдены

 ### 8.2 Ограничения и проблемы

**Дублирование данных:**
- Halo-строки дублируются между соседними процессами
- Дополнительная память: O(width × num_processes)

**Дисбаланс нагрузки:**
- При height не кратном числу процессов возможен небольшой дисбаланс
- Максимальная разница: 1 строка между процессами


## 9. Источники

1. Лекции Сысоева А.В.
2. Материалы курса, ppc-2025-processes-engineers
https://github.com/learning-process/ppc-2025-processes-engineers
3. Курс компьютерной графики 2 курс 5 семестр 

## 10. Приложение

### MPI-реализация(ключевой алгоритм)

```cpp
bool LazarevaAGaussFilterHorizontalMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int rows_per_proc = height_ / size;
  int remainder = height_ % size;
  
  std::vector<int> rows_count(size);
  std::vector<int> rows_offset(size);
  
  int offset = 0;
  for (int i = 0; i < size; i++) {
    rows_count[i] = rows_per_proc + (i < remainder ? 1 : 0);
    rows_offset[i] = offset;
    offset += rows_count[i];
  }
  
  int local_rows = rows_count[rank];
  int local_start_row = rows_offset[rank];
  
  int halo_top = (local_start_row > 0) ? 1 : 0;
  int halo_bottom = (local_start_row + local_rows < height_) ? 1 : 0;
  int extended_rows = local_rows + halo_top + halo_bottom;
  
  std::vector<int> sendcounts(size);
  std::vector<int> displs(size);
  
  for (int i = 0; i < size; i++) {
    int start = rows_offset[i];
    int count = rows_count[i];
    int htop = (start > 0) ? 1 : 0;
    int hbot = (start + count < height_) ? 1 : 0;
    
    sendcounts[i] = (count + htop + hbot) * width_;
    displs[i] = (start - htop) * width_;
  }
  
  std::vector<int> local_data(extended_rows * width_);
  
  MPI_Scatterv(rank == 0 ? GetInput().data() + 2 : nullptr,
               sendcounts.data(), displs.data(), MPI_INT,
               local_data.data(), extended_rows * width_, MPI_INT,
               0, MPI_COMM_WORLD);
  
  std::vector<int> local_result(local_rows * width_);
  
  for (int i = 0; i < local_rows; i++) {
    int ext_i = i + halo_top;
    
    for (int j = 0; j < width_; j++) {
      int sum = 0;
      
      for (int ki = -1; ki <= 1; ki++) {
        for (int kj = -1; kj <= 1; kj++) {
          int row = ext_i + ki;
          int col = j + kj;
          
          if (row < 0) row = 0;
          if (row >= extended_rows) row = extended_rows - 1;
          if (col < 0) col = 0;
          if (col >= width_) col = width_ - 1;
          
          sum += local_data[row * width_ + col] * kernel_[ki + 1][kj + 1];
        }
      }
      
      local_result[i * width_ + j] = sum / kernel_sum_;
    }
  }
  
  std::vector<int> recvcounts(size);
  std::vector<int> recvdispls(size);
  
  offset = 0;
  for (int i = 0; i < size; i++) {
    recvcounts[i] = rows_count[i] * width_;
    recvdispls[i] = offset;
    offset += recvcounts[i];
  }
  
  MPI_Gatherv(local_result.data(), local_rows * width_, MPI_INT,
              rank == 0 ? GetOutput().data() : nullptr,
              recvcounts.data(), recvdispls.data(), MPI_INT,
              0, MPI_COMM_WORLD);

  return true;
}

```